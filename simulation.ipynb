{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=20250306)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_problem(n=64, k=5):\n",
    "    props = rng.uniform(size=k)\n",
    "    return rng.choice(k, size=n, p=props/props.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import sys\n",
    "\n",
    "import pydantic\n",
    "import langchain_openai\n",
    "import langchain_core\n",
    "import tqdm.auto\n",
    "import pandas as pd\n",
    "\n",
    "from consol.confidence_models import AbstractConfidenceModel, SbftConfidenceModel, SprtConfidenceModel, PValueConfidenceModel, BayesianConfidenceModel, VoteConfidenceModel\n",
    "\n",
    "class ConfidentSolverConfig(pydantic.BaseModel):\n",
    "    max_trials: int\n",
    "\n",
    "class ConfidentSolver:\n",
    "    def __init__(\n",
    "        self,\n",
    "        confidence_model: typing.Union[str, AbstractConfidenceModel],\n",
    "        max_trials=64,\n",
    "    ):\n",
    "        self.config = ConfidentSolverConfig(\n",
    "            max_trials=max_trials,\n",
    "        )\n",
    "        if confidence_model == \"sbft\":\n",
    "            self.confidence_model = SbftConfidenceModel()\n",
    "        elif confidence_model == \"sprt\":\n",
    "            self.confidence_model = SprtConfidenceModel()\n",
    "        elif confidence_model == \"pvalue\":\n",
    "            self.confidence_model = PValueConfidenceModel()\n",
    "        elif confidence_model == \"bayesian\":\n",
    "            self.confidence_model = BayesianConfidenceModel()\n",
    "        elif confidence_model == \"vote\":\n",
    "            self.confidence_model = VoteConfidenceModel()\n",
    "        elif isinstance(confidence_model, AbstractConfidenceModel):\n",
    "            self.confidence_model = confidence_model\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown Confidence Model: {confidence_model}\")\n",
    "\n",
    "    def invoke(self, answers, debug=False):\n",
    "        max_trials = self.config.max_trials\n",
    "\n",
    "        total_raw_outputs = []\n",
    "        total_raw_outputs.append(answers[0]) \n",
    "        i = 1\n",
    "\n",
    "        total_invoke = 0\n",
    "        while True:     \n",
    "            total_invoke += 1       \n",
    "            first, second = self._get_top_two_answers(total_raw_outputs)\n",
    "            trials = self._determine_trials(first, second, max_trials, len(total_raw_outputs))\n",
    "            if trials == 0:\n",
    "                break\n",
    "            for j in range(trials):\n",
    "                total_raw_outputs.append(answers[i])\n",
    "                i += 1\n",
    "        df = self._create_dataframe(total_raw_outputs)\n",
    "        if debug:\n",
    "            return df\n",
    "        return {\"target\": df['answer'].mode().iloc[0], \"total_invoke\": total_invoke, \"total_runs\": len(total_raw_outputs)}\n",
    "\n",
    "    def _get_top_two_answers(self, total_raw_outputs):\n",
    "        total_ss = pd.Series(total_raw_outputs).value_counts()\n",
    "        two = total_ss.sort_values(ascending=False).head(2).to_list()\n",
    "        while len(two) < 2:\n",
    "            two += [0]\n",
    "        return two[0], two[1]\n",
    "\n",
    "    def _determine_trials(self, first, second, max_trials, current_trials):\n",
    "        for trials in range(0, max_trials + 1):\n",
    "            if first + trials == 0:\n",
    "                continue\n",
    "            if self.confidence_model.test(first + trials, second):\n",
    "                break\n",
    "        if trials >= max_trials - current_trials:\n",
    "            trials = max_trials - current_trials\n",
    "        return trials\n",
    "\n",
    "    def _create_dataframe(self, total_raw_outputs):\n",
    "        return pd.DataFrame({\n",
    "            'answer': total_raw_outputs,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:55<00:00, 180.21it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_EXPR = 10_000\n",
    "MAX_ITER = 64\n",
    "\n",
    "records = []\n",
    "for i in tqdm.tqdm(range(MAX_EXPR)):\n",
    "    problem = generate_random_problem(k=2)\n",
    "    answer = np.bincount(problem).argmax()\n",
    "\n",
    "    for solver_name in [\"sbft\", \"sprt\", \"pvalue\",\"bayesian\"]:\n",
    "        solver = ConfidentSolver(solver_name)\n",
    "        ret = solver.invoke(problem)\n",
    "        ret[\"solver\"] = solver_name\n",
    "        ret[\"answer\"] = answer\n",
    "        ret[\"correct\"] = ret[\"answer\"] == ret[\"target\"]\n",
    "        records.append(ret)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k2 = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solver\n",
       "bayesian    0.0342\n",
       "pvalue      0.0155\n",
       "sbft        0.0003\n",
       "sprt        0.0519\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-df_k2.groupby(\"solver\")[\"correct\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solver\n",
       "bayesian    22.1943\n",
       "pvalue      26.8065\n",
       "sbft        45.7483\n",
       "sprt        34.9943\n",
       "Name: total_runs, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_k2.groupby(\"solver\")[\"total_runs\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:30<00:00, 110.05it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_EXPR = 10_000\n",
    "MAX_ITER = 64\n",
    "\n",
    "records = []\n",
    "for i in tqdm.tqdm(range(MAX_EXPR)):\n",
    "    problem = generate_random_problem(k=5)\n",
    "    answer = np.bincount(problem).argmax()\n",
    "\n",
    "    for solver_name in [\"sbft\", \"sprt\", \"pvalue\",\"bayesian\"]:\n",
    "        solver = ConfidentSolver(solver_name)\n",
    "        ret = solver.invoke(problem)\n",
    "        ret[\"solver\"] = solver_name\n",
    "        ret[\"answer\"] = answer\n",
    "        ret[\"correct\"] = ret[\"answer\"] == ret[\"target\"]\n",
    "        records.append(ret)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_k5 = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solver\n",
       "bayesian    0.0216\n",
       "pvalue      0.0066\n",
       "sbft        0.0000\n",
       "sprt        0.0848\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-df_k5.groupby(\"solver\")[\"correct\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solver\n",
       "bayesian    50.4819\n",
       "pvalue      54.7092\n",
       "sbft        63.3170\n",
       "sprt        58.1474\n",
       "Name: total_runs, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_k5.groupby(\"solver\")[\"total_runs\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:16<00:00, 620.41it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_EXPR = 10_000\n",
    "MAX_ITER = 64\n",
    "\n",
    "records = []\n",
    "for i in tqdm.tqdm(range(MAX_EXPR)):\n",
    "    problem = generate_random_problem(k=1)\n",
    "    answer = np.bincount(problem).argmax()\n",
    "\n",
    "    for solver_name in [\"sbft\", \"sprt\", \"pvalue\",\"bayesian\"]:\n",
    "        solver = ConfidentSolver(solver_name)\n",
    "        ret = solver.invoke(problem)\n",
    "        ret[\"solver\"] = solver_name\n",
    "        ret[\"answer\"] = answer\n",
    "        ret[\"correct\"] = ret[\"answer\"] == ret[\"target\"]\n",
    "        records.append(ret)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solver\n",
       "bayesian    0.0\n",
       "pvalue      0.0\n",
       "sbft        0.0\n",
       "sprt        0.0\n",
       "Name: correct, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_k1 = pd.DataFrame(records)\n",
    "1-df_k1.groupby(\"solver\")[\"correct\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "solver\n",
       "bayesian    4.0\n",
       "pvalue      5.0\n",
       "sbft        9.0\n",
       "sprt        9.0\n",
       "Name: total_runs, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_k1.groupby(\"solver\")[\"total_runs\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
